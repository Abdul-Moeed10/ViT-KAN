{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1bbfO3aHRdE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['KAGGLE_USERNAME'] = 'yourkaggleusername'\n",
        "os.environ['KAGGLE_KEY'] = 'yourkaggleapikey'\n",
        "\n",
        "# Download HAR dataset\n",
        "!kaggle datasets download -d meetnagadia/human-action-recognition-har-dataset --unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-SwBKSNHddP",
        "outputId": "87149ad8-e0f8-407a-916f-219491290d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/meetnagadia/human-action-recognition-har-dataset\n",
            "License(s): ODbL-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KindXiaoming/pykan.git\n",
        "!pip install ./pykan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9P0vxIpHh8q",
        "outputId": "bd751580-ac90-4b1f-dca7-04da51dad2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pykan'...\n",
            "remote: Enumerating objects: 4221, done.\u001b[K\n",
            "remote: Counting objects: 100% (661/661), done.\u001b[K\n",
            "remote: Compressing objects: 100% (240/240), done.\u001b[K\n",
            "remote: Total 4221 (delta 573), reused 421 (delta 421), pack-reused 3560 (from 3)\u001b[K\n",
            "Receiving objects: 100% (4221/4221), 114.77 MiB | 45.25 MiB/s, done.\n",
            "Resolving deltas: 100% (1580/1580), done.\n",
            "Processing ./pykan\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pykan\n",
            "  Building wheel for pykan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykan: filename=pykan-0.2.8-py3-none-any.whl size=78235 sha256=b02e7596740392097b5d754ea11f94bda6c94df865f5267a081758bcf5fa2796\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r1cn7mc4/wheels/05/9b/6c/6f9f5a9927ba27c99b92cf0cbdd57f190932c31289c49eded1\n",
            "Successfully built pykan\n",
            "Installing collected packages: pykan\n",
            "Successfully installed pykan-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/Human Action Recognition'\n",
        "csv_path = os.path.join(data_path, 'Training_set.csv')"
      ],
      "metadata": {
        "id": "aK6807m9Hjrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HARCSVImageDataset(Dataset):\n",
        "    def __init__(self, root_dir, csv_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "\n",
        "        df = pd.read_csv(csv_file)\n",
        "        for _, row in df.iterrows():\n",
        "            img_path = os.path.join(self.root_dir, 'train', row['filename'])\n",
        "            if os.path.isfile(img_path):\n",
        "                self.data.append(img_path)\n",
        "                self.labels.append(row['label'])\n",
        "\n",
        "        self.label_mapping = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n",
        "        self.labels = [self.label_mapping[label] for label in self.labels]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n"
      ],
      "metadata": {
        "id": "wXwwyDywHnO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = HARCSVImageDataset(root_dir=data_path, csv_file=csv_path, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=2)  # Reuse same CSV for test\n"
      ],
      "metadata": {
        "id": "E1IjJwPDHo-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pykan.kan import KANLayer\n",
        "\n",
        "# ViT-KAN model definition\n",
        "vit_weights = models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
        "vit = models.vit_b_16(weights=vit_weights)\n",
        "\n",
        "for param in vit.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# KAN classification head\n",
        "class KANHead(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dims, num_classes):\n",
        "        super(KANHead, self).__init__()\n",
        "        self.norm = nn.LayerNorm(embedding_dim)\n",
        "        self.kan1 = KANLayer(in_dim=embedding_dim, out_dim=hidden_dims[0])\n",
        "        self.kan2 = KANLayer(in_dim=hidden_dims[0], out_dim=hidden_dims[1])\n",
        "        self.kan3 = KANLayer(in_dim=hidden_dims[1], out_dim=hidden_dims[2])\n",
        "        self.out = KANLayer(in_dim=hidden_dims[2], out_dim=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        x = self.kan1(x)[0]\n",
        "        x = F.relu(x)\n",
        "        x = self.kan2(x)[0]\n",
        "        x = F.relu(x)\n",
        "        x = self.kan3(x)[0]\n",
        "        x = F.relu(x)\n",
        "        x = self.out(x)[0]\n",
        "        return x\n",
        "\n",
        "vit.heads = KANHead(embedding_dim=768, hidden_dims=[128, 64, 32], num_classes=len(train_dataset.label_mapping))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5It4k7WHquO",
        "outputId": "efa053d7-1aa5-4b23-9ed6-2df85ddea426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16_swag-9ac1b537.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16_swag-9ac1b537.pth\n",
            "100%|██████████| 331M/331M [00:01<00:00, 215MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "vit.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJb9OQi-Hta1",
        "outputId": "daa064cc-5600-48e1-d39f-102728ec277e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): KANHead(\n",
              "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (kan1): KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "    (kan2): KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "    (kan3): KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "    (out): KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vit.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "metadata": {
        "id": "tuIF0EdcHvzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Epoch {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}] Loss: {loss.item():.4f}')\n",
        "    return running_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "O1Pf0YL6HxN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1)\n",
        "            correct += pred.eq(target).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest Loss: {test_loss:.4f}, Accuracy: {accuracy:.2f}%\\n')\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "phiqIcGlHzIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "    train_loss = train(vit, device, train_loader, optimizer, epoch)\n",
        "    test_loss = evaluate(vit, device, test_loader)\n",
        "    scheduler.step(test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz_xTo7eH0hC",
        "outputId": "51b5e1a6-deb3-439c-c200-66d587b622b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 [0/12600] Loss: 2.7097\n",
            "Epoch 0 [320/12600] Loss: 2.7070\n",
            "Epoch 0 [640/12600] Loss: 2.6772\n",
            "Epoch 0 [960/12600] Loss: 2.5766\n",
            "Epoch 0 [1280/12600] Loss: 2.2665\n",
            "Epoch 0 [1600/12600] Loss: 1.9802\n",
            "Epoch 0 [1920/12600] Loss: 1.8342\n",
            "Epoch 0 [2240/12600] Loss: 1.5472\n",
            "Epoch 0 [2560/12600] Loss: 1.5649\n",
            "Epoch 0 [2880/12600] Loss: 1.2773\n",
            "Epoch 0 [3200/12600] Loss: 1.3831\n",
            "Epoch 0 [3520/12600] Loss: 1.0775\n",
            "Epoch 0 [3840/12600] Loss: 0.9282\n",
            "Epoch 0 [4160/12600] Loss: 1.0041\n",
            "Epoch 0 [4480/12600] Loss: 0.7744\n",
            "Epoch 0 [4800/12600] Loss: 1.1212\n",
            "Epoch 0 [5120/12600] Loss: 1.0406\n",
            "Epoch 0 [5440/12600] Loss: 0.7520\n",
            "Epoch 0 [5760/12600] Loss: 0.9868\n",
            "Epoch 0 [6080/12600] Loss: 1.2061\n",
            "Epoch 0 [6400/12600] Loss: 0.7668\n",
            "Epoch 0 [6720/12600] Loss: 0.8787\n",
            "Epoch 0 [7040/12600] Loss: 0.6573\n",
            "Epoch 0 [7360/12600] Loss: 0.7075\n",
            "Epoch 0 [7680/12600] Loss: 0.9792\n",
            "Epoch 0 [8000/12600] Loss: 0.9607\n",
            "Epoch 0 [8320/12600] Loss: 0.5484\n",
            "Epoch 0 [8640/12600] Loss: 0.7819\n",
            "Epoch 0 [8960/12600] Loss: 0.2990\n",
            "Epoch 0 [9280/12600] Loss: 0.9960\n",
            "Epoch 0 [9600/12600] Loss: 0.9832\n",
            "Epoch 0 [9920/12600] Loss: 0.4598\n",
            "Epoch 0 [10240/12600] Loss: 0.7614\n",
            "Epoch 0 [10560/12600] Loss: 0.8740\n",
            "Epoch 0 [10880/12600] Loss: 0.7281\n",
            "Epoch 0 [11200/12600] Loss: 0.6583\n",
            "Epoch 0 [11520/12600] Loss: 0.5304\n",
            "Epoch 0 [11840/12600] Loss: 0.6935\n",
            "Epoch 0 [12160/12600] Loss: 0.7950\n",
            "Epoch 0 [12480/12600] Loss: 0.6423\n",
            "\n",
            "Test Loss: 0.0186, Accuracy: 81.71%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 [0/12600] Loss: 0.3558\n",
            "Epoch 1 [320/12600] Loss: 0.5088\n",
            "Epoch 1 [640/12600] Loss: 0.3287\n",
            "Epoch 1 [960/12600] Loss: 0.6450\n",
            "Epoch 1 [1280/12600] Loss: 0.2953\n",
            "Epoch 1 [1600/12600] Loss: 0.5228\n",
            "Epoch 1 [1920/12600] Loss: 0.5685\n",
            "Epoch 1 [2240/12600] Loss: 0.5311\n",
            "Epoch 1 [2560/12600] Loss: 0.3088\n",
            "Epoch 1 [2880/12600] Loss: 0.5532\n",
            "Epoch 1 [3200/12600] Loss: 0.4052\n",
            "Epoch 1 [3520/12600] Loss: 0.6004\n",
            "Epoch 1 [3840/12600] Loss: 0.3198\n",
            "Epoch 1 [4160/12600] Loss: 0.8032\n",
            "Epoch 1 [4480/12600] Loss: 0.4545\n",
            "Epoch 1 [4800/12600] Loss: 0.3839\n",
            "Epoch 1 [5120/12600] Loss: 0.6890\n",
            "Epoch 1 [5440/12600] Loss: 0.4581\n",
            "Epoch 1 [5760/12600] Loss: 0.5380\n",
            "Epoch 1 [6080/12600] Loss: 0.6418\n",
            "Epoch 1 [6400/12600] Loss: 0.9909\n",
            "Epoch 1 [6720/12600] Loss: 0.5877\n",
            "Epoch 1 [7040/12600] Loss: 0.5184\n",
            "Epoch 1 [7360/12600] Loss: 0.5475\n",
            "Epoch 1 [7680/12600] Loss: 0.4848\n",
            "Epoch 1 [8000/12600] Loss: 0.9601\n",
            "Epoch 1 [8320/12600] Loss: 1.1520\n",
            "Epoch 1 [8640/12600] Loss: 0.5965\n",
            "Epoch 1 [8960/12600] Loss: 0.6424\n",
            "Epoch 1 [9280/12600] Loss: 0.6718\n",
            "Epoch 1 [9600/12600] Loss: 0.5732\n",
            "Epoch 1 [9920/12600] Loss: 0.5714\n",
            "Epoch 1 [10240/12600] Loss: 0.6266\n",
            "Epoch 1 [10560/12600] Loss: 0.5450\n",
            "Epoch 1 [10880/12600] Loss: 0.3118\n",
            "Epoch 1 [11200/12600] Loss: 0.4373\n",
            "Epoch 1 [11520/12600] Loss: 0.7961\n",
            "Epoch 1 [11840/12600] Loss: 0.4818\n",
            "Epoch 1 [12160/12600] Loss: 0.9214\n",
            "Epoch 1 [12480/12600] Loss: 0.6294\n",
            "\n",
            "Test Loss: 0.0147, Accuracy: 85.67%\n",
            "\n",
            "Epoch 2 [0/12600] Loss: 0.5254\n",
            "Epoch 2 [320/12600] Loss: 0.7153\n",
            "Epoch 2 [640/12600] Loss: 0.4200\n",
            "Epoch 2 [960/12600] Loss: 0.3680\n",
            "Epoch 2 [1280/12600] Loss: 0.2564\n",
            "Epoch 2 [1600/12600] Loss: 0.4518\n",
            "Epoch 2 [1920/12600] Loss: 0.4288\n",
            "Epoch 2 [2240/12600] Loss: 0.4814\n",
            "Epoch 2 [2560/12600] Loss: 0.3709\n",
            "Epoch 2 [2880/12600] Loss: 0.1851\n",
            "Epoch 2 [3200/12600] Loss: 0.7439\n",
            "Epoch 2 [3520/12600] Loss: 0.8822\n",
            "Epoch 2 [3840/12600] Loss: 0.6631\n",
            "Epoch 2 [4160/12600] Loss: 0.5623\n",
            "Epoch 2 [4480/12600] Loss: 0.6655\n",
            "Epoch 2 [4800/12600] Loss: 0.3297\n",
            "Epoch 2 [5120/12600] Loss: 0.6966\n",
            "Epoch 2 [5440/12600] Loss: 0.2795\n",
            "Epoch 2 [5760/12600] Loss: 0.4675\n",
            "Epoch 2 [6080/12600] Loss: 0.2334\n",
            "Epoch 2 [6400/12600] Loss: 0.3858\n",
            "Epoch 2 [6720/12600] Loss: 0.4663\n",
            "Epoch 2 [7040/12600] Loss: 0.4040\n",
            "Epoch 2 [7360/12600] Loss: 0.5784\n",
            "Epoch 2 [7680/12600] Loss: 0.3657\n",
            "Epoch 2 [8000/12600] Loss: 0.6342\n",
            "Epoch 2 [8320/12600] Loss: 0.7140\n",
            "Epoch 2 [8640/12600] Loss: 0.6533\n",
            "Epoch 2 [8960/12600] Loss: 0.5439\n",
            "Epoch 2 [9280/12600] Loss: 0.6741\n",
            "Epoch 2 [9600/12600] Loss: 0.6378\n",
            "Epoch 2 [9920/12600] Loss: 0.7704\n",
            "Epoch 2 [10240/12600] Loss: 0.2985\n",
            "Epoch 2 [10560/12600] Loss: 0.4362\n",
            "Epoch 2 [10880/12600] Loss: 0.2321\n",
            "Epoch 2 [11200/12600] Loss: 0.8225\n",
            "Epoch 2 [11520/12600] Loss: 0.8551\n",
            "Epoch 2 [11840/12600] Loss: 0.6128\n",
            "Epoch 2 [12160/12600] Loss: 0.4704\n",
            "Epoch 2 [12480/12600] Loss: 0.3393\n",
            "\n",
            "Test Loss: 0.0117, Accuracy: 88.33%\n",
            "\n",
            "Epoch 3 [0/12600] Loss: 0.3674\n",
            "Epoch 3 [320/12600] Loss: 0.3684\n",
            "Epoch 3 [640/12600] Loss: 0.2322\n",
            "Epoch 3 [960/12600] Loss: 0.2744\n",
            "Epoch 3 [1280/12600] Loss: 0.3029\n",
            "Epoch 3 [1600/12600] Loss: 0.3067\n",
            "Epoch 3 [1920/12600] Loss: 0.4403\n",
            "Epoch 3 [2240/12600] Loss: 0.4252\n",
            "Epoch 3 [2560/12600] Loss: 0.4282\n",
            "Epoch 3 [2880/12600] Loss: 0.3876\n",
            "Epoch 3 [3200/12600] Loss: 0.4017\n",
            "Epoch 3 [3520/12600] Loss: 0.6061\n",
            "Epoch 3 [3840/12600] Loss: 0.4779\n",
            "Epoch 3 [4160/12600] Loss: 0.4645\n",
            "Epoch 3 [4480/12600] Loss: 0.2812\n",
            "Epoch 3 [4800/12600] Loss: 0.6645\n",
            "Epoch 3 [5120/12600] Loss: 0.5906\n",
            "Epoch 3 [5440/12600] Loss: 0.3957\n",
            "Epoch 3 [5760/12600] Loss: 0.3666\n",
            "Epoch 3 [6080/12600] Loss: 0.2804\n",
            "Epoch 3 [6400/12600] Loss: 0.2560\n",
            "Epoch 3 [6720/12600] Loss: 0.3559\n",
            "Epoch 3 [7040/12600] Loss: 0.4084\n",
            "Epoch 3 [7360/12600] Loss: 0.3363\n",
            "Epoch 3 [7680/12600] Loss: 0.7509\n",
            "Epoch 3 [8000/12600] Loss: 0.5582\n",
            "Epoch 3 [8320/12600] Loss: 0.5721\n",
            "Epoch 3 [8640/12600] Loss: 0.1776\n",
            "Epoch 3 [8960/12600] Loss: 0.3287\n",
            "Epoch 3 [9280/12600] Loss: 0.3000\n",
            "Epoch 3 [9600/12600] Loss: 0.5028\n",
            "Epoch 3 [9920/12600] Loss: 0.4325\n",
            "Epoch 3 [10240/12600] Loss: 0.1910\n",
            "Epoch 3 [10560/12600] Loss: 0.4375\n",
            "Epoch 3 [10880/12600] Loss: 0.3822\n",
            "Epoch 3 [11200/12600] Loss: 0.1935\n",
            "Epoch 3 [11520/12600] Loss: 0.3009\n",
            "Epoch 3 [11840/12600] Loss: 0.7076\n",
            "Epoch 3 [12160/12600] Loss: 0.5684\n",
            "Epoch 3 [12480/12600] Loss: 0.3742\n",
            "\n",
            "Test Loss: 0.0101, Accuracy: 89.67%\n",
            "\n",
            "Epoch 4 [0/12600] Loss: 0.1391\n",
            "Epoch 4 [320/12600] Loss: 0.1570\n",
            "Epoch 4 [640/12600] Loss: 0.0873\n",
            "Epoch 4 [960/12600] Loss: 0.1485\n",
            "Epoch 4 [1280/12600] Loss: 0.2472\n",
            "Epoch 4 [1600/12600] Loss: 0.3051\n",
            "Epoch 4 [1920/12600] Loss: 0.2738\n",
            "Epoch 4 [2240/12600] Loss: 0.2758\n",
            "Epoch 4 [2560/12600] Loss: 0.3084\n",
            "Epoch 4 [2880/12600] Loss: 0.3626\n",
            "Epoch 4 [3200/12600] Loss: 0.3633\n",
            "Epoch 4 [3520/12600] Loss: 0.3310\n",
            "Epoch 4 [3840/12600] Loss: 0.4590\n",
            "Epoch 4 [4160/12600] Loss: 0.3548\n",
            "Epoch 4 [4480/12600] Loss: 0.3586\n",
            "Epoch 4 [4800/12600] Loss: 0.4627\n",
            "Epoch 4 [5120/12600] Loss: 0.2417\n",
            "Epoch 4 [5440/12600] Loss: 0.3070\n",
            "Epoch 4 [5760/12600] Loss: 0.2837\n",
            "Epoch 4 [6080/12600] Loss: 0.5081\n",
            "Epoch 4 [6400/12600] Loss: 0.3599\n",
            "Epoch 4 [6720/12600] Loss: 0.5898\n",
            "Epoch 4 [7040/12600] Loss: 0.1197\n",
            "Epoch 4 [7360/12600] Loss: 0.2757\n",
            "Epoch 4 [7680/12600] Loss: 0.2184\n",
            "Epoch 4 [8000/12600] Loss: 0.3134\n",
            "Epoch 4 [8320/12600] Loss: 0.1877\n",
            "Epoch 4 [8640/12600] Loss: 0.1514\n",
            "Epoch 4 [8960/12600] Loss: 0.1826\n",
            "Epoch 4 [9280/12600] Loss: 0.4957\n",
            "Epoch 4 [9600/12600] Loss: 0.3348\n",
            "Epoch 4 [9920/12600] Loss: 0.2505\n",
            "Epoch 4 [10240/12600] Loss: 0.3843\n",
            "Epoch 4 [10560/12600] Loss: 0.5000\n",
            "Epoch 4 [10880/12600] Loss: 0.3461\n",
            "Epoch 4 [11200/12600] Loss: 0.1489\n",
            "Epoch 4 [11520/12600] Loss: 0.2814\n",
            "Epoch 4 [11840/12600] Loss: 0.3128\n",
            "Epoch 4 [12160/12600] Loss: 0.3712\n",
            "Epoch 4 [12480/12600] Loss: 0.2910\n",
            "\n",
            "Test Loss: 0.0084, Accuracy: 91.52%\n",
            "\n",
            "Epoch 5 [0/12600] Loss: 0.2925\n",
            "Epoch 5 [320/12600] Loss: 0.2259\n",
            "Epoch 5 [640/12600] Loss: 0.1672\n",
            "Epoch 5 [960/12600] Loss: 0.1178\n",
            "Epoch 5 [1280/12600] Loss: 0.2410\n",
            "Epoch 5 [1600/12600] Loss: 0.4066\n",
            "Epoch 5 [1920/12600] Loss: 0.1028\n",
            "Epoch 5 [2240/12600] Loss: 0.3456\n",
            "Epoch 5 [2560/12600] Loss: 0.2061\n",
            "Epoch 5 [2880/12600] Loss: 0.2559\n",
            "Epoch 5 [3200/12600] Loss: 0.1692\n",
            "Epoch 5 [3520/12600] Loss: 0.4303\n",
            "Epoch 5 [3840/12600] Loss: 0.2468\n",
            "Epoch 5 [4160/12600] Loss: 0.1937\n",
            "Epoch 5 [4480/12600] Loss: 0.5716\n",
            "Epoch 5 [4800/12600] Loss: 0.2313\n",
            "Epoch 5 [5120/12600] Loss: 0.3856\n",
            "Epoch 5 [5440/12600] Loss: 0.3625\n",
            "Epoch 5 [5760/12600] Loss: 0.1917\n",
            "Epoch 5 [6080/12600] Loss: 0.3164\n",
            "Epoch 5 [6400/12600] Loss: 0.2274\n",
            "Epoch 5 [6720/12600] Loss: 0.2042\n",
            "Epoch 5 [7040/12600] Loss: 0.1706\n",
            "Epoch 5 [7360/12600] Loss: 0.1398\n",
            "Epoch 5 [7680/12600] Loss: 0.6397\n",
            "Epoch 5 [8000/12600] Loss: 0.1732\n",
            "Epoch 5 [8320/12600] Loss: 0.3764\n",
            "Epoch 5 [8640/12600] Loss: 0.2778\n",
            "Epoch 5 [8960/12600] Loss: 0.2352\n",
            "Epoch 5 [9280/12600] Loss: 0.2056\n",
            "Epoch 5 [9600/12600] Loss: 0.3818\n",
            "Epoch 5 [9920/12600] Loss: 0.2675\n",
            "Epoch 5 [10240/12600] Loss: 0.3759\n",
            "Epoch 5 [10560/12600] Loss: 0.3728\n",
            "Epoch 5 [10880/12600] Loss: 0.2402\n",
            "Epoch 5 [11200/12600] Loss: 0.4367\n",
            "Epoch 5 [11520/12600] Loss: 0.6930\n",
            "Epoch 5 [11840/12600] Loss: 0.2859\n",
            "Epoch 5 [12160/12600] Loss: 0.2351\n",
            "Epoch 5 [12480/12600] Loss: 0.2279\n",
            "\n",
            "Test Loss: 0.0059, Accuracy: 94.48%\n",
            "\n",
            "Epoch 6 [0/12600] Loss: 0.1373\n",
            "Epoch 6 [320/12600] Loss: 0.1577\n",
            "Epoch 6 [640/12600] Loss: 0.2282\n",
            "Epoch 6 [960/12600] Loss: 0.1632\n",
            "Epoch 6 [1280/12600] Loss: 0.1838\n",
            "Epoch 6 [1600/12600] Loss: 0.1959\n",
            "Epoch 6 [1920/12600] Loss: 0.1175\n",
            "Epoch 6 [2240/12600] Loss: 0.2720\n",
            "Epoch 6 [2560/12600] Loss: 0.1746\n",
            "Epoch 6 [2880/12600] Loss: 0.1402\n",
            "Epoch 6 [3200/12600] Loss: 0.2896\n",
            "Epoch 6 [3520/12600] Loss: 0.2792\n",
            "Epoch 6 [3840/12600] Loss: 0.2579\n",
            "Epoch 6 [4160/12600] Loss: 0.3665\n",
            "Epoch 6 [4480/12600] Loss: 0.1284\n",
            "Epoch 6 [4800/12600] Loss: 0.1618\n",
            "Epoch 6 [5120/12600] Loss: 0.1142\n",
            "Epoch 6 [5440/12600] Loss: 0.1894\n",
            "Epoch 6 [5760/12600] Loss: 0.1068\n",
            "Epoch 6 [6080/12600] Loss: 0.1180\n",
            "Epoch 6 [6400/12600] Loss: 0.3073\n",
            "Epoch 6 [6720/12600] Loss: 0.2281\n",
            "Epoch 6 [7040/12600] Loss: 0.3785\n",
            "Epoch 6 [7360/12600] Loss: 0.0839\n",
            "Epoch 6 [7680/12600] Loss: 0.1920\n",
            "Epoch 6 [8000/12600] Loss: 0.1230\n",
            "Epoch 6 [8320/12600] Loss: 0.2472\n",
            "Epoch 6 [8640/12600] Loss: 0.2771\n",
            "Epoch 6 [8960/12600] Loss: 0.1110\n",
            "Epoch 6 [9280/12600] Loss: 0.1228\n",
            "Epoch 6 [9600/12600] Loss: 0.1198\n",
            "Epoch 6 [9920/12600] Loss: 0.1608\n",
            "Epoch 6 [10240/12600] Loss: 0.2846\n",
            "Epoch 6 [10560/12600] Loss: 0.2297\n",
            "Epoch 6 [10880/12600] Loss: 0.4012\n",
            "Epoch 6 [11200/12600] Loss: 0.1658\n",
            "Epoch 6 [11520/12600] Loss: 0.2199\n",
            "Epoch 6 [11840/12600] Loss: 0.2350\n",
            "Epoch 6 [12160/12600] Loss: 0.2165\n",
            "Epoch 6 [12480/12600] Loss: 0.2727\n",
            "\n",
            "Test Loss: 0.0047, Accuracy: 95.50%\n",
            "\n",
            "Epoch 7 [0/12600] Loss: 0.0373\n",
            "Epoch 7 [320/12600] Loss: 0.1769\n",
            "Epoch 7 [640/12600] Loss: 0.0951\n",
            "Epoch 7 [960/12600] Loss: 0.0420\n",
            "Epoch 7 [1280/12600] Loss: 0.3411\n",
            "Epoch 7 [1600/12600] Loss: 0.2681\n",
            "Epoch 7 [1920/12600] Loss: 0.0370\n",
            "Epoch 7 [2240/12600] Loss: 0.1663\n",
            "Epoch 7 [2560/12600] Loss: 0.1854\n",
            "Epoch 7 [2880/12600] Loss: 0.0952\n",
            "Epoch 7 [3200/12600] Loss: 0.0787\n",
            "Epoch 7 [3520/12600] Loss: 0.1639\n",
            "Epoch 7 [3840/12600] Loss: 0.0677\n",
            "Epoch 7 [4160/12600] Loss: 0.0665\n",
            "Epoch 7 [4480/12600] Loss: 0.1396\n",
            "Epoch 7 [4800/12600] Loss: 0.1030\n",
            "Epoch 7 [5120/12600] Loss: 0.0626\n",
            "Epoch 7 [5440/12600] Loss: 0.3170\n",
            "Epoch 7 [5760/12600] Loss: 0.2354\n",
            "Epoch 7 [6080/12600] Loss: 0.1929\n",
            "Epoch 7 [6400/12600] Loss: 0.1194\n",
            "Epoch 7 [6720/12600] Loss: 0.2176\n",
            "Epoch 7 [7040/12600] Loss: 0.0950\n",
            "Epoch 7 [7360/12600] Loss: 0.1789\n",
            "Epoch 7 [7680/12600] Loss: 0.2666\n",
            "Epoch 7 [8000/12600] Loss: 0.2515\n",
            "Epoch 7 [8320/12600] Loss: 0.1098\n",
            "Epoch 7 [8640/12600] Loss: 0.1114\n",
            "Epoch 7 [8960/12600] Loss: 0.1816\n",
            "Epoch 7 [9280/12600] Loss: 0.1297\n",
            "Epoch 7 [9600/12600] Loss: 0.0770\n",
            "Epoch 7 [9920/12600] Loss: 0.1594\n",
            "Epoch 7 [10240/12600] Loss: 0.0978\n",
            "Epoch 7 [10560/12600] Loss: 0.1156\n",
            "Epoch 7 [10880/12600] Loss: 0.3088\n",
            "Epoch 7 [11200/12600] Loss: 0.0842\n",
            "Epoch 7 [11520/12600] Loss: 0.1841\n",
            "Epoch 7 [11840/12600] Loss: 0.0900\n",
            "Epoch 7 [12160/12600] Loss: 0.0793\n",
            "Epoch 7 [12480/12600] Loss: 0.1950\n",
            "\n",
            "Test Loss: 0.0037, Accuracy: 96.53%\n",
            "\n",
            "Epoch 8 [0/12600] Loss: 0.2073\n",
            "Epoch 8 [320/12600] Loss: 0.1664\n",
            "Epoch 8 [640/12600] Loss: 0.0544\n",
            "Epoch 8 [960/12600] Loss: 0.3649\n",
            "Epoch 8 [1280/12600] Loss: 0.0627\n",
            "Epoch 8 [1600/12600] Loss: 0.0616\n",
            "Epoch 8 [1920/12600] Loss: 0.2553\n",
            "Epoch 8 [2240/12600] Loss: 0.0255\n",
            "Epoch 8 [2560/12600] Loss: 0.1206\n",
            "Epoch 8 [2880/12600] Loss: 0.1050\n",
            "Epoch 8 [3200/12600] Loss: 0.1389\n",
            "Epoch 8 [3520/12600] Loss: 0.0294\n",
            "Epoch 8 [3840/12600] Loss: 0.0953\n",
            "Epoch 8 [4160/12600] Loss: 0.0249\n",
            "Epoch 8 [4480/12600] Loss: 0.0830\n",
            "Epoch 8 [4800/12600] Loss: 0.0304\n",
            "Epoch 8 [5120/12600] Loss: 0.1473\n",
            "Epoch 8 [5440/12600] Loss: 0.1846\n",
            "Epoch 8 [5760/12600] Loss: 0.1559\n",
            "Epoch 8 [6080/12600] Loss: 0.1254\n",
            "Epoch 8 [6400/12600] Loss: 0.1096\n",
            "Epoch 8 [6720/12600] Loss: 0.1188\n",
            "Epoch 8 [7040/12600] Loss: 0.0784\n",
            "Epoch 8 [7360/12600] Loss: 0.1910\n",
            "Epoch 8 [7680/12600] Loss: 0.1351\n",
            "Epoch 8 [8000/12600] Loss: 0.1360\n",
            "Epoch 8 [8320/12600] Loss: 0.0709\n",
            "Epoch 8 [8640/12600] Loss: 0.1677\n",
            "Epoch 8 [8960/12600] Loss: 0.0442\n",
            "Epoch 8 [9280/12600] Loss: 0.0773\n",
            "Epoch 8 [9600/12600] Loss: 0.0210\n",
            "Epoch 8 [9920/12600] Loss: 0.0723\n",
            "Epoch 8 [10240/12600] Loss: 0.0960\n",
            "Epoch 8 [10560/12600] Loss: 0.2168\n",
            "Epoch 8 [10880/12600] Loss: 0.2813\n",
            "Epoch 8 [11200/12600] Loss: 0.0731\n",
            "Epoch 8 [11520/12600] Loss: 0.1550\n",
            "Epoch 8 [11840/12600] Loss: 0.0701\n",
            "Epoch 8 [12160/12600] Loss: 0.0588\n",
            "Epoch 8 [12480/12600] Loss: 0.0723\n",
            "\n",
            "Test Loss: 0.0025, Accuracy: 98.05%\n",
            "\n",
            "Epoch 9 [0/12600] Loss: 0.0384\n",
            "Epoch 9 [320/12600] Loss: 0.0761\n",
            "Epoch 9 [640/12600] Loss: 0.0246\n",
            "Epoch 9 [960/12600] Loss: 0.2317\n",
            "Epoch 9 [1280/12600] Loss: 0.1012\n",
            "Epoch 9 [1600/12600] Loss: 0.0412\n",
            "Epoch 9 [1920/12600] Loss: 0.0387\n",
            "Epoch 9 [2240/12600] Loss: 0.0799\n",
            "Epoch 9 [2560/12600] Loss: 0.2978\n",
            "Epoch 9 [2880/12600] Loss: 0.1037\n",
            "Epoch 9 [3200/12600] Loss: 0.1236\n",
            "Epoch 9 [3520/12600] Loss: 0.0149\n",
            "Epoch 9 [3840/12600] Loss: 0.0497\n",
            "Epoch 9 [4160/12600] Loss: 0.0311\n",
            "Epoch 9 [4480/12600] Loss: 0.1515\n",
            "Epoch 9 [4800/12600] Loss: 0.1312\n",
            "Epoch 9 [5120/12600] Loss: 0.2166\n",
            "Epoch 9 [5440/12600] Loss: 0.0923\n",
            "Epoch 9 [5760/12600] Loss: 0.0493\n",
            "Epoch 9 [6080/12600] Loss: 0.0257\n",
            "Epoch 9 [6400/12600] Loss: 0.1382\n",
            "Epoch 9 [6720/12600] Loss: 0.0811\n",
            "Epoch 9 [7040/12600] Loss: 0.0740\n",
            "Epoch 9 [7360/12600] Loss: 0.1238\n",
            "Epoch 9 [7680/12600] Loss: 0.1231\n",
            "Epoch 9 [8000/12600] Loss: 0.0375\n",
            "Epoch 9 [8320/12600] Loss: 0.0838\n",
            "Epoch 9 [8640/12600] Loss: 0.1284\n",
            "Epoch 9 [8960/12600] Loss: 0.3048\n",
            "Epoch 9 [9280/12600] Loss: 0.0150\n",
            "Epoch 9 [9600/12600] Loss: 0.0644\n",
            "Epoch 9 [9920/12600] Loss: 0.0612\n",
            "Epoch 9 [10240/12600] Loss: 0.0908\n",
            "Epoch 9 [10560/12600] Loss: 0.1674\n",
            "Epoch 9 [10880/12600] Loss: 0.0360\n",
            "Epoch 9 [11200/12600] Loss: 0.0165\n",
            "Epoch 9 [11520/12600] Loss: 0.1688\n",
            "Epoch 9 [11840/12600] Loss: 0.1074\n",
            "Epoch 9 [12160/12600] Loss: 0.2149\n",
            "Epoch 9 [12480/12600] Loss: 0.1343\n",
            "\n",
            "Test Loss: 0.0015, Accuracy: 98.98%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(model, device, loader):\n",
        "    model.eval()\n",
        "    all_targets, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for data, targets in loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            outputs = model(data)\n",
        "            preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "            all_preds.extend(preds)\n",
        "            all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_targets, all_preds)\n",
        "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
        "    mae = mean_absolute_error(all_targets, all_preds)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    return accuracy, rmse, mae"
      ],
      "metadata": {
        "id": "v4rg4_J1H2Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_metrics(vit, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqOeOkyvH5oZ",
        "outputId": "61c205e5-2f20-42dc-8c43-d487d7ee3038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 98.98%\n",
            "RMSE: 0.7081\n",
            "MAE: 0.0592\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9898412698412699, np.float64(0.7081162132224988), 0.05920634920634921)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uG1xND6LH7DW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}