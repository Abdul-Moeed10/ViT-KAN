{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_T_CoID4wdx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip the Penn Action dataset from Google Drive\n",
        "data_zip_path = '/content/drive/My Drive/data.zip'  # Path to your data.zip in Google Drive\n",
        "extract_path = '/content/data'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "with zipfile.ZipFile(data_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRNoQ99j4190",
        "outputId": "6e7a3fb9-8db5-42a3-b9db-be4dbefefab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_mapping = {\n",
        "    'tennis_serve': 0, 'golf_swing': 1, 'baseball_pitch': 2, 'bench_press': 3,\n",
        "    'pullup': 4, 'pushup': 5, 'situp': 6, 'jumping_jacks': 7, 'strum_guitar': 8,\n",
        "    'bowl': 9, 'tennis_forehand': 10, 'squat': 11, 'jump_rope': 12,\n",
        "    'clean_and_jerk': 13, 'baseball_swing': 14\n",
        "}"
      ],
      "metadata": {
        "id": "MkR7KAi-49bF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PennActionDataset(Dataset):\n",
        "    def __init__(self, data_path, transform=None, num_frames=32):\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.num_frames = num_frames\n",
        "        self.frames_dir = os.path.join(data_path, \"frames\")\n",
        "        self.labels_dir = os.path.join(data_path, \"labels\")\n",
        "        self.video_ids = os.listdir(self.frames_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_id = self.video_ids[idx]\n",
        "        video_path = os.path.join(self.frames_dir, video_id)\n",
        "        label_path = os.path.join(self.labels_dir, video_id + \".mat\")\n",
        "        frame_paths = sorted([os.path.join(video_path, f) for f in os.listdir(video_path)])\n",
        "        frame_count = len(frame_paths)\n",
        "        if frame_count > self.num_frames:\n",
        "            selected_indices = np.linspace(0, frame_count - 1, self.num_frames).astype(int)\n",
        "            frame_paths = [frame_paths[i] for i in selected_indices]\n",
        "        elif frame_count < self.num_frames:\n",
        "            frame_paths += [frame_paths[-1]] * (self.num_frames - frame_count)\n",
        "        frames = [Image.open(frame_path).convert(\"RGB\") for frame_path in frame_paths]\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "        frames = torch.stack(frames)\n",
        "        mat = scipy.io.loadmat(label_path)\n",
        "        action_label = mat[\"action\"][0]\n",
        "        label = label_mapping[action_label]\n",
        "        return frames, label\n"
      ],
      "metadata": {
        "id": "MYqxeg5X5Tek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),  # Resize to 384x384 for SWAG weights\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalize\n",
        "])"
      ],
      "metadata": {
        "id": "GVcE_sY85V3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PennActionDataset('/content/data/data/Penn_Action/Penn_Action', transform=transform, num_frames=32)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "t-q25BZF5bOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KindXiaoming/pykan.git\n",
        "!pip install ./pykan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujj-6ULPELjF",
        "outputId": "4eadc9ef-381d-491b-9690-a2f857c3ac12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pykan'...\n",
            "remote: Enumerating objects: 4221, done.\u001b[K\n",
            "remote: Counting objects: 100% (664/664), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 4221 (delta 569), reused 419 (delta 419), pack-reused 3557 (from 3)\u001b[K\n",
            "Receiving objects: 100% (4221/4221), 114.76 MiB | 45.71 MiB/s, done.\n",
            "Resolving deltas: 100% (1580/1580), done.\n",
            "Processing ./pykan\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pykan\n",
            "  Building wheel for pykan (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pykan: filename=pykan-0.2.8-py3-none-any.whl size=78235 sha256=a2906fea05b2c84c62f396c529daa16758da6ff9d373eb104df583f18ca662eb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-s70g9nqp/wheels/05/9b/6c/6f9f5a9927ba27c99b92cf0cbdd57f190932c31289c49eded1\n",
            "Successfully built pykan\n",
            "Installing collected packages: pykan\n",
            "Successfully installed pykan-0.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit_weights = models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
        "pretrained_vit = models.vit_b_16(weights=vit_weights)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define KAN Layer\n",
        "class KANLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, activation='relu'):\n",
        "        super(KANLayer, self).__init__()\n",
        "        self.weights = nn.Parameter(torch.randn(out_dim, in_dim))\n",
        "        self.bias = nn.Parameter(torch.zeros(out_dim))\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.matmul(x, self.weights.T) + self.bias\n",
        "        if self.activation == 'relu':\n",
        "            return F.relu(x)\n",
        "        elif self.activation == 'tanh':\n",
        "            return torch.tanh(x)\n",
        "        elif self.activation == 'sigmoid':\n",
        "            return torch.sigmoid(x)\n",
        "        return x  # No activation if None\n",
        "\n",
        "# Freeze pretrained weights\n",
        "for param in pretrained_vit.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Update the classification head with custom KAN layers\n",
        "embedding_dim = 768  # Dimension of ViT embeddings\n",
        "kan_hidden_units = [64, 32]  # Define hidden layers for KAN\n",
        "\n",
        "pretrained_vit.heads = nn.Sequential(\n",
        "    nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "    KANLayer(in_dim=embedding_dim, out_dim=kan_hidden_units[0], activation='relu'),\n",
        "    KANLayer(in_dim=kan_hidden_units[0], out_dim=kan_hidden_units[1], activation='relu'),\n",
        "    KANLayer(in_dim=kan_hidden_units[1], out_dim=len(label_mapping), activation=None)\n",
        ")"
      ],
      "metadata": {
        "id": "VslKcScR5f1I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6b19030-0e40-44df-cd91-4d18f63a01db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16_swag-9ac1b537.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16_swag-9ac1b537.pth\n",
            "100%|██████████| 331M/331M [00:02<00:00, 154MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pykan\n",
        "print(dir(pykan))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G--2frMqWlVh",
        "outputId": "bca30704-49a5-4d8e-8e05-ff87c3bc49e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pykan.kan\n",
        "print(dir(pykan.kan))  # If 'layers' exist, check its content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngoKR0PaWuqA",
        "outputId": "9c646aa8-3487-4e41-f323-3121dceadeee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Abs', 'AccumBounds', 'Add', 'Adjoint', 'AlgebraicField', 'AlgebraicNumber', 'And', 'AppliedPredicate', 'Array', 'AssumptionsContext', 'Atom', 'AtomicExpr', 'BasePolynomialError', 'Basic', 'BlockDiagMatrix', 'BlockMatrix', 'CC', 'CRootOf', 'Catalan', 'Chi', 'Ci', 'Circle', 'CoercionFailed', 'Complement', 'ComplexField', 'ComplexRegion', 'ComplexRootOf', 'Complexes', 'ComputationFailed', 'ConditionSet', 'Contains', 'CosineTransform', 'Curve', 'DeferredVector', 'DenseNDimArray', 'Derivative', 'Determinant', 'DiagMatrix', 'DiagonalMatrix', 'DiagonalOf', 'Dict', 'DiracDelta', 'DisjointUnion', 'Domain', 'DomainError', 'DotProduct', 'Dummy', 'E', 'E1', 'EPath', 'EX', 'EXRAW', 'Ei', 'Eijk', 'Ellipse', 'EmptySequence', 'EmptySet', 'Eq', 'Equality', 'Equivalent', 'EulerGamma', 'EvaluationFailed', 'ExactQuotientFailed', 'Expr', 'ExpressionDomain', 'ExtraneousFactors', 'FF', 'FF_gmpy', 'FF_python', 'FU', 'FallingFactorial', 'FiniteField', 'FiniteSet', 'FlagError', 'Float', 'FourierTransform', 'FractionField', 'Function', 'FunctionClass', 'FunctionMatrix', 'GF', 'GMPYFiniteField', 'GMPYIntegerRing', 'GMPYRationalField', 'Ge', 'GeneratorsError', 'GeneratorsNeeded', 'GeometryError', 'GoldenRatio', 'GramSchmidt', 'GreaterThan', 'GroebnerBasis', 'Gt', 'HadamardPower', 'HadamardProduct', 'HankelTransform', 'Heaviside', 'HeuristicGCDFailed', 'HomomorphismFailed', 'I', 'ITE', 'Id', 'Identity', 'Idx', 'ImageSet', 'ImmutableDenseMatrix', 'ImmutableDenseNDimArray', 'ImmutableMatrix', 'ImmutableSparseMatrix', 'ImmutableSparseNDimArray', 'Implies', 'Indexed', 'IndexedBase', 'Integer', 'IntegerRing', 'Integers', 'Integral', 'Intersection', 'Interval', 'Inverse', 'InverseCosineTransform', 'InverseFourierTransform', 'InverseHankelTransform', 'InverseLaplaceTransform', 'InverseMellinTransform', 'InverseSineTransform', 'IsomorphismFailed', 'KAN', 'KANLayer', 'KroneckerDelta', 'KroneckerProduct', 'LBFGS', 'LC', 'LM', 'LT', 'Lambda', 'LambertW', 'LaplaceTransform', 'Le', 'LessThan', 'LeviCivita', 'Li', 'Limit', 'Line', 'Line2D', 'Line3D', 'LinearRegression', 'Lt', 'MatAdd', 'MatMul', 'MatPow', 'Matrix', 'MatrixBase', 'MatrixExpr', 'MatrixPermute', 'MatrixSlice', 'MatrixSymbol', 'Max', 'MellinTransform', 'Min', 'Mod', 'Monomial', 'Mul', 'MultKAN', 'MultivariatePolynomialError', 'MutableDenseMatrix', 'MutableDenseNDimArray', 'MutableMatrix', 'MutableSparseMatrix', 'MutableSparseNDimArray', 'N', 'NDimArray', 'Nand', 'Naturals', 'Naturals0', 'Ne', 'NonSquareMatrixError', 'Nor', 'Not', 'NotAlgebraic', 'NotInvertible', 'NotReversible', 'Number', 'NumberSymbol', 'O', 'OmegaPower', 'OneMatrix', 'OperationNotSupported', 'OptionError', 'Options', 'Or', 'Order', 'Ordinal', 'POSform', 'Parabola', 'Permanent', 'PermutationMatrix', 'Piecewise', 'Plane', 'Point', 'Point2D', 'Point3D', 'PoleError', 'PolificationFailed', 'Poly', 'Polygon', 'PolynomialDivisionFailed', 'PolynomialError', 'PolynomialRing', 'Pow', 'PowerSet', 'PrecisionExhausted', 'Predicate', 'Product', 'ProductSet', 'PurePoly', 'PythonFiniteField', 'PythonIntegerRing', 'PythonRational', 'Q', 'QQ', 'QQ_I', 'QQ_gmpy', 'QQ_python', 'Quaternion', 'RR', 'Range', 'Rational', 'RationalField', 'Rationals', 'Ray', 'Ray2D', 'Ray3D', 'RealField', 'RealNumber', 'Reals', 'RefinementFailed', 'RegularPolygon', 'Rel', 'Rem', 'RisingFactorial', 'RootOf', 'RootSum', 'S', 'SOPform', 'SYMBOLIC_LIB', 'Segment', 'Segment2D', 'Segment3D', 'SeqAdd', 'SeqFormula', 'SeqMul', 'SeqPer', 'Set', 'ShapeError', 'Shi', 'Si', 'Sieve', 'SineTransform', 'SingularityFunction', 'SparseMatrix', 'SparseNDimArray', 'StrPrinter', 'StrictGreaterThan', 'StrictLessThan', 'Subs', 'Sum', 'Symbol', 'Symbolic_KANLayer', 'SymmetricDifference', 'SympifyError', 'TableForm', 'Trace', 'Transpose', 'Triangle', 'TribonacciConstant', 'Tuple', 'Unequality', 'UnevaluatedExpr', 'UnificationFailed', 'Union', 'UnivariatePolynomialError', 'UniversalSet', 'Wild', 'WildFunction', 'Xor', 'Ynm', 'Ynm_c', 'ZZ', 'ZZ_I', 'ZZ_gmpy', 'ZZ_python', 'ZeroMatrix', 'Znm', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'abundance', 'acos', 'acosh', 'acot', 'acoth', 'acsc', 'acsch', 'add_symbolic', 'adjoint', 'airyai', 'airyaiprime', 'airybi', 'airybiprime', 'algebras', 'all_roots', 'andre', 'apart', 'apart_list', 'appellf1', 'apply_finite_diff', 'approximants', 'are_similar', 'arg', 'arity', 'asec', 'asech', 'asin', 'asinh', 'ask', 'assemble_partfrac_list', 'assoc_laguerre', 'assoc_legendre', 'assuming', 'assumptions', 'atan', 'atan2', 'atanh', 'augment_input', 'banded', 'batch_hessian', 'batch_jacobian', 'bell', 'bernoulli', 'besseli', 'besselj', 'besselk', 'besselsimp', 'bessely', 'beta', 'betainc', 'betainc_regularized', 'binomial', 'binomial_coefficients', 'binomial_coefficients_list', 'block_collapse', 'blockcut', 'bool_map', 'bottom_up', 'bspline_basis', 'bspline_basis_set', 'cacheit', 'calculus', 'cancel', 'capture', 'carmichael', 'cartes', 'casoratian', 'catalan', 'cbrt', 'ccode', 'ceiling', 'centroid', 'chebyshevt', 'chebyshevt_poly', 'chebyshevt_root', 'chebyshevu', 'chebyshevu_poly', 'chebyshevu_root', 'check_assumptions', 'checkodesol', 'checkpdesol', 'checksol', 'classify_ode', 'classify_pde', 'closest_points', 'cofactors', 'collect', 'collect_const', 'combsimp', 'comp', 'compose', 'composite', 'compositepi', 'concrete', 'conjugate', 'construct_domain', 'content', 'continued_fraction', 'continued_fraction_convergents', 'continued_fraction_iterator', 'continued_fraction_periodic', 'continued_fraction_reduce', 'convex_hull', 'convolution', 'copy', 'cos', 'cosh', 'cosine_transform', 'cot', 'coth', 'count_ops', 'count_roots', 'covering_product', 'create_dataset', 'create_dataset_from_data', 'csc', 'csch', 'cse', 'curve2coef', 'cxxcode', 'cycle_length', 'cyclotomic_poly', 'decompogen', 'decompose', 'default_sort_key', 'deg', 'degree', 'degree_list', 'denom', 'derive_by_array', 'det', 'det_quick', 'diag', 'diagonalize_vector', 'dict_merge', 'diff', 'difference_delta', 'differentiate_finite', 'digamma', 'diophantine', 'dirichlet_eta', 'discrete', 'discrete_log', 'discriminant', 'div', 'divisor_count', 'divisor_sigma', 'divisors', 'doctest', 'dotprint', 'dsolve', 'egyptian_fraction', 'elliptic_e', 'elliptic_f', 'elliptic_k', 'elliptic_pi', 'epath', 'erf', 'erf2', 'erf2inv', 'erfc', 'erfcinv', 'erfi', 'erfinv', 'euler', 'euler_equations', 'evalf', 'evaluate', 'ex_round', 'exp', 'exp_polar', 'expand', 'expand_complex', 'expand_func', 'expand_log', 'expand_mul', 'expand_multinomial', 'expand_power_base', 'expand_power_exp', 'expand_trig', 'expint', 'exptrigsimp', 'exquo', 'external', 'eye', 'f_arccos', 'f_arcsin', 'f_arctanh', 'f_exp', 'f_inv', 'f_inv2', 'f_inv3', 'f_inv4', 'f_inv5', 'f_invsqrt', 'f_log', 'f_power1d5', 'f_sqrt', 'f_tan', 'factor', 'factor_list', 'factor_nc', 'factor_terms', 'factorial', 'factorial2', 'factorint', 'factorrat', 'failing_assumptions', 'false', 'farthest_points', 'fcode', 'ff', 'fft', 'fibonacci', 'field', 'field_isomorphism', 'filldedent', 'finite_diff_weights', 'fit_params', 'flatten', 'floor', 'fourier_series', 'fourier_transform', 'fps', 'frac', 'fraction', 'fresnelc', 'fresnels', 'fu', 'functions', 'fwht', 'galois_group', 'gamma', 'gammasimp', 'gcd', 'gcd_list', 'gcd_terms', 'gcdex', 'gegenbauer', 'genocchi', 'geometry', 'get_contraction_structure', 'get_derivative', 'get_indices', 'gff', 'gff_list', 'glob', 'glsl_code', 'grevlex', 'grlex', 'groebner', 'ground_roots', 'group', 'gruntz', 'hadamard_product', 'half_gcdex', 'hankel1', 'hankel2', 'hankel_transform', 'harmonic', 'has_dups', 'has_variety', 'hermite', 'hermite_poly', 'hermite_prob', 'hermite_prob_poly', 'hessian', 'hn1', 'hn2', 'homogeneous_order', 'horner', 'hyper', 'hyperexpand', 'hypersimilar', 'hypersimp', 'hypothesis', 'idiff', 'ifft', 'ifwht', 'igcd', 'igrevlex', 'igrlex', 'ilcm', 'ilex', 'im', 'imageset', 'init_printing', 'init_session', 'integer_log', 'integer_nthroot', 'integrate', 'interactive', 'interactive_traversal', 'interpolate', 'interpolating_poly', 'interpolating_spline', 'intersecting_product', 'intersection', 'intervals', 'intt', 'inv_quick', 'inverse_cosine_transform', 'inverse_fourier_transform', 'inverse_hankel_transform', 'inverse_laplace_transform', 'inverse_mellin_transform', 'inverse_mobius_transform', 'inverse_sine_transform', 'invert', 'is_abundant', 'is_amicable', 'is_carmichael', 'is_convex', 'is_decreasing', 'is_deficient', 'is_increasing', 'is_mersenne_prime', 'is_monotonic', 'is_nthpow_residue', 'is_perfect', 'is_primitive_root', 'is_quad_residue', 'is_strictly_decreasing', 'is_strictly_increasing', 'is_zero_dimensional', 'isolate', 'isprime', 'itermonomials', 'jacobi', 'jacobi_normalized', 'jacobi_poly', 'jacobi_symbol', 'jn', 'jn_zeros', 'jordan_cell', 'jscode', 'julia_code', 'kronecker_product', 'kronecker_symbol', 'kroneckersimp', 'laguerre', 'laguerre_poly', 'lambdify', 'laplace_correspondence', 'laplace_initial_conds', 'laplace_transform', 'latex', 'lcm', 'lcm_list', 'legendre', 'legendre_poly', 'legendre_symbol', 'lerchphi', 'lex', 'li', 'limit', 'limit_seq', 'line_integrate', 'linear_eq_to_matrix', 'linsolve', 'list2numpy', 'ln', 'log', 'logcombine', 'loggamma', 'lowergamma', 'lucas', 'maple_code', 'marcumq', 'mathematica_code', 'mathieuc', 'mathieucprime', 'mathieus', 'mathieusprime', 'mathml', 'matrix2numpy', 'matrix_multiply_elementwise', 'matrix_symbols', 'maximum', 'meijerg', 'mellin_transform', 'memoize_property', 'mersenne_prime_exponent', 'minimal_polynomial', 'minimum', 'minpoly', 'mobius', 'mobius_transform', 'mod_inverse', 'model2param', 'monic', 'motzkin', 'multigamma', 'multiline_latex', 'multinomial_coefficients', 'multipledispatch', 'multiplicity', 'n_order', 'nan', 'nextprime', 'nfloat', 'nn', 'nonlinsolve', 'not_empty_in', 'np', 'npartitions', 'nroots', 'nsimplify', 'nsolve', 'nth_power_roots_poly', 'ntheory', 'nthroot_mod', 'ntt', 'num_digits', 'numbered_symbols', 'numer', 'octave_code', 'ode_order', 'ones', 'oo', 'ord0', 'ordered', 'os', 'pager_print', 'parallel_poly_from_expr', 'parse_expr', 'parsing', 'partition', 'pd', 'pde_separate', 'pde_separate_add', 'pde_separate_mul', 'pdiv', 'pdsolve', 'per', 'perfect_power', 'periodic_argument', 'periodicity', 'permutedims', 'pexquo', 'pi', 'piecewise_exclusive', 'piecewise_fold', 'plot', 'plot_backends', 'plot_implicit', 'plot_parametric', 'plot_tree', 'plotting', 'plt', 'polar_lift', 'polarify', 'pollard_pm1', 'pollard_rho', 'poly', 'poly_from_expr', 'polygamma', 'polylog', 'polys', 'posify', 'postfixes', 'postorder_traversal', 'powdenest', 'powsimp', 'pprint', 'pprint_try_use_unicode', 'pprint_use_unicode', 'pquo', 'prefixes', 'prem', 'preorder_traversal', 'pretty', 'pretty_print', 'preview', 'prevprime', 'prime', 'prime_decomp', 'prime_valuation', 'primefactors', 'primenu', 'primeomega', 'primepi', 'primerange', 'primitive', 'primitive_element', 'primitive_root', 'primorial', 'principal_branch', 'print_ccode', 'print_fcode', 'print_glsl', 'print_gtk', 'print_jscode', 'print_latex', 'print_maple_code', 'print_mathml', 'print_python', 'print_rcode', 'print_tree', 'printing', 'prod', 'product', 'proper_divisor_count', 'proper_divisors', 'public', 'pycode', 'python', 'quadratic_congruence', 'quadratic_residues', 'quo', 'rad', 'radsimp', 'randMatrix', 'random', 'random_poly', 'randprime', 'rational_interpolate', 'ratsimp', 'ratsimpmodprime', 'rcode', 'rcollect', 're', 'real_root', 'real_roots', 'reduce_abs_inequalities', 'reduce_abs_inequality', 'reduce_inequalities', 'reduced', 'reduced_totient', 'refine', 'refine_root', 'register_handler', 'release', 'rem', 'remove_handler', 'reshape', 'residue', 'resultant', 'rf', 'riemann_xi', 'ring', 'root', 'rootof', 'roots', 'rot_axis1', 'rot_axis2', 'rot_axis3', 'rot_ccw_axis1', 'rot_ccw_axis2', 'rot_ccw_axis3', 'rot_givens', 'rotations', 'round_two', 'rsolve', 'rsolve_hyper', 'rsolve_poly', 'rsolve_ratio', 'rust_code', 'satisfiable', 'sec', 'sech', 'separatevars', 'sequence', 'series', 'seterr', 'sfield', 'shape', 'sieve', 'sift', 'sign', 'signsimp', 'simplify', 'simplify_logic', 'sin', 'sinc', 'sine_transform', 'singularities', 'singularityintegrate', 'sinh', 'smtlib_code', 'solve', 'solve_linear', 'solve_linear_system', 'solve_linear_system_LU', 'solve_poly_inequality', 'solve_poly_system', 'solve_rational_inequalities', 'solve_triangulated', 'solve_undetermined_coeffs', 'solve_univariate_inequality', 'solveset', 'sparse_mask', 'spline', 'sqf', 'sqf_list', 'sqf_norm', 'sqf_part', 'sqrt', 'sqrt_mod', 'sqrt_mod_iter', 'sqrtdenest', 'srepr', 'sring', 'sstr', 'sstrrepr', 'stationary_points', 'stieltjes', 'strategies', 'sturm', 'subfactorial', 'subresultants', 'subsets', 'substitution', 'summation', 'swinnerton_dyer_poly', 'symarray', 'symbols', 'symmetric_poly', 'symmetrize', 'sympify', 'sympy', 'take', 'tan', 'tanh', 'tensor', 'tensorcontraction', 'tensordiagonal', 'tensorproduct', 'terms_gcd', 'test', 'textplot', 'threaded', 'timed', 'to_cnf', 'to_dnf', 'to_nnf', 'to_number_field', 'together', 'topological_sort', 'torch', 'total_degree', 'totient', 'tqdm', 'trace', 'trailing', 'transpose', 'tribonacci', 'trigamma', 'trigsimp', 'true', 'trunc', 'unbranched_argument', 'unflatten', 'unpolarify', 'uppergamma', 'use', 'utilities', 'utils', 'var', 'variations', 'vectorize', 'vfield', 'viete', 'vring', 'wronskian', 'xfield', 'xring', 'xthreaded', 'yaml', 'yn', 'zeros', 'zeta', 'zoo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "pretrained_vit.to(device)"
      ],
      "metadata": {
        "id": "MDU1H3Wu5iHV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611912f2-317e-4e1e-dfbb-14dc6fdb7e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (encoder): Encoder(\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "    (layers): Sequential(\n",
              "      (encoder_layer_0): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_1): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_2): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_3): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_4): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_5): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_6): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_7): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_8): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_9): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_10): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (encoder_layer_11): EncoderBlock(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (self_attention): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU(approximate='none')\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (4): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (heads): KANHead(\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (kan1): KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "    (kan2): KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "    (kan3): KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "    (kan_out): KANLayer(\n",
              "      (base_fun): SiLU()\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pykan\n",
        "from pykan.kan import KANLayer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Freeze pretrained weights\n",
        "for param in pretrained_vit.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Update the classification head with pykan KAN layers\n",
        "embedding_dim = 768  # Dimension of ViT embeddings\n",
        "kan_hidden_units = [128, 64, 32]  # Increased model capacity for better learning\n",
        "\n",
        "class KANHead(nn.Module):\n",
        "    def __init__(self, embedding_dim, kan_hidden_units, num_classes):\n",
        "        super(KANHead, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "        self.kan1 = KANLayer(in_dim=embedding_dim, out_dim=kan_hidden_units[0])\n",
        "        self.kan2 = KANLayer(in_dim=kan_hidden_units[0], out_dim=kan_hidden_units[1])\n",
        "        self.kan3 = KANLayer(in_dim=kan_hidden_units[1], out_dim=kan_hidden_units[2])\n",
        "        self.kan_out = KANLayer(in_dim=kan_hidden_units[2], out_dim=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.kan1(x)[0]  # Extract only the tensor\n",
        "        x = F.relu(x)\n",
        "        x = self.kan2(x)[0]\n",
        "        x = F.relu(x)\n",
        "        x = self.kan3(x)[0]\n",
        "        x = F.relu(x)\n",
        "        x = self.kan_out(x)[0]  # Extract only the tensor\n",
        "        return x\n",
        "\n",
        "pretrained_vit.heads = KANHead(embedding_dim, kan_hidden_units, len(label_mapping))\n",
        "pretrained_vit.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(pretrained_vit.parameters(), lr=5e-4, weight_decay=1e-5)  # Lower learning rate for stability\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.7)  # Adjusted scheduler for better convergence"
      ],
      "metadata": {
        "id": "DwRIe7eJWR5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(pretrained_vit.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)"
      ],
      "metadata": {
        "id": "IWH6xS2L5kXT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    pretrained_vit.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (frames, labels) in enumerate(dataloader):\n",
        "        frames = frames.mean(dim=1).to(device)  # Average frames across the time dimension\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = pretrained_vit(frames)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item():.4f}\")\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {total_loss / len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNBil9qj6Rw8",
        "outputId": "27717587-83ad-4631-ea07-d8678ba608f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Batch 10/73, Loss: 2.6983\n",
            "Epoch 1/10, Batch 20/73, Loss: 2.5328\n",
            "Epoch 1/10, Batch 30/73, Loss: 2.3363\n",
            "Epoch 1/10, Batch 40/73, Loss: 1.9419\n",
            "Epoch 1/10, Batch 50/73, Loss: 1.6446\n",
            "Epoch 1/10, Batch 60/73, Loss: 1.4655\n",
            "Epoch 1/10, Batch 70/73, Loss: 1.4562\n",
            "Epoch 1/10, Average Loss: 2.0725\n",
            "Epoch 2/10, Batch 10/73, Loss: 1.0318\n",
            "Epoch 2/10, Batch 20/73, Loss: 1.1978\n",
            "Epoch 2/10, Batch 30/73, Loss: 0.8471\n",
            "Epoch 2/10, Batch 40/73, Loss: 0.8779\n",
            "Epoch 2/10, Batch 50/73, Loss: 0.8052\n",
            "Epoch 2/10, Batch 60/73, Loss: 0.6011\n",
            "Epoch 2/10, Batch 70/73, Loss: 0.9108\n",
            "Epoch 2/10, Average Loss: 0.9741\n",
            "Epoch 3/10, Batch 10/73, Loss: 0.5722\n",
            "Epoch 3/10, Batch 20/73, Loss: 0.7375\n",
            "Epoch 3/10, Batch 30/73, Loss: 0.7700\n",
            "Epoch 3/10, Batch 40/73, Loss: 0.5065\n",
            "Epoch 3/10, Batch 50/73, Loss: 0.3851\n",
            "Epoch 3/10, Batch 60/73, Loss: 0.6137\n",
            "Epoch 3/10, Batch 70/73, Loss: 0.7443\n",
            "Epoch 3/10, Average Loss: 0.6347\n",
            "Epoch 4/10, Batch 10/73, Loss: 0.3347\n",
            "Epoch 4/10, Batch 20/73, Loss: 0.4752\n",
            "Epoch 4/10, Batch 30/73, Loss: 0.6984\n",
            "Epoch 4/10, Batch 40/73, Loss: 0.4133\n",
            "Epoch 4/10, Batch 50/73, Loss: 0.5630\n",
            "Epoch 4/10, Batch 60/73, Loss: 0.3549\n",
            "Epoch 4/10, Batch 70/73, Loss: 0.4221\n",
            "Epoch 4/10, Average Loss: 0.4640\n",
            "Epoch 5/10, Batch 10/73, Loss: 0.4532\n",
            "Epoch 5/10, Batch 20/73, Loss: 0.3026\n",
            "Epoch 5/10, Batch 30/73, Loss: 0.4202\n",
            "Epoch 5/10, Batch 40/73, Loss: 0.2370\n",
            "Epoch 5/10, Batch 50/73, Loss: 0.3524\n",
            "Epoch 5/10, Batch 60/73, Loss: 0.4202\n",
            "Epoch 5/10, Batch 70/73, Loss: 0.1943\n",
            "Epoch 5/10, Average Loss: 0.3290\n",
            "Epoch 6/10, Batch 10/73, Loss: 0.2528\n",
            "Epoch 6/10, Batch 20/73, Loss: 0.1568\n",
            "Epoch 6/10, Batch 30/73, Loss: 0.2059\n",
            "Epoch 6/10, Batch 40/73, Loss: 0.2120\n",
            "Epoch 6/10, Batch 50/73, Loss: 0.0977\n",
            "Epoch 6/10, Batch 60/73, Loss: 0.2431\n",
            "Epoch 6/10, Batch 70/73, Loss: 0.1077\n",
            "Epoch 6/10, Average Loss: 0.2228\n",
            "Epoch 7/10, Batch 10/73, Loss: 0.1672\n",
            "Epoch 7/10, Batch 20/73, Loss: 0.1616\n",
            "Epoch 7/10, Batch 30/73, Loss: 0.1170\n",
            "Epoch 7/10, Batch 40/73, Loss: 0.1546\n",
            "Epoch 7/10, Batch 50/73, Loss: 0.1756\n",
            "Epoch 7/10, Batch 60/73, Loss: 0.1705\n",
            "Epoch 7/10, Batch 70/73, Loss: 0.2539\n",
            "Epoch 7/10, Average Loss: 0.1814\n",
            "Epoch 8/10, Batch 10/73, Loss: 0.1306\n",
            "Epoch 8/10, Batch 20/73, Loss: 0.1122\n",
            "Epoch 8/10, Batch 30/73, Loss: 0.1731\n",
            "Epoch 8/10, Batch 40/73, Loss: 0.0856\n",
            "Epoch 8/10, Batch 50/73, Loss: 0.1315\n",
            "Epoch 8/10, Batch 60/73, Loss: 0.1495\n",
            "Epoch 8/10, Batch 70/73, Loss: 0.0757\n",
            "Epoch 8/10, Average Loss: 0.1448\n",
            "Epoch 9/10, Batch 10/73, Loss: 0.0799\n",
            "Epoch 9/10, Batch 20/73, Loss: 0.0819\n",
            "Epoch 9/10, Batch 30/73, Loss: 0.2845\n",
            "Epoch 9/10, Batch 40/73, Loss: 0.0872\n",
            "Epoch 9/10, Batch 50/73, Loss: 0.3745\n",
            "Epoch 9/10, Batch 60/73, Loss: 0.2237\n",
            "Epoch 9/10, Batch 70/73, Loss: 0.0952\n",
            "Epoch 9/10, Average Loss: 0.1187\n",
            "Epoch 10/10, Batch 10/73, Loss: 0.0467\n",
            "Epoch 10/10, Batch 20/73, Loss: 0.1380\n",
            "Epoch 10/10, Batch 30/73, Loss: 0.1909\n",
            "Epoch 10/10, Batch 40/73, Loss: 0.0746\n",
            "Epoch 10/10, Batch 50/73, Loss: 0.0825\n",
            "Epoch 10/10, Batch 60/73, Loss: 0.0582\n",
            "Epoch 10/10, Batch 70/73, Loss: 0.0781\n",
            "Epoch 10/10, Average Loss: 0.0971\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_vit.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for frames, labels in dataloader:\n",
        "        frames = frames.mean(dim=1).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = pretrained_vit(frames)\n",
        "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
        "        all_preds.extend(preds)\n",
        "        all_labels.extend(labels.cpu().numpy())"
      ],
      "metadata": {
        "id": "oYKbro6D6S3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "rmse = np.sqrt(mean_squared_error(all_labels, all_preds))\n",
        "mae = mean_absolute_error(all_labels, all_preds)\n",
        "\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"MAE: {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrqgjMYJNgKx",
        "outputId": "868abed7-d170-4f1f-d060-e9a066190c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 99.23%\n",
            "RMSE: 0.6421\n",
            "MAE: 0.0477\n"
          ]
        }
      ]
    }
  ]
}